{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization\n",
    "\n",
    "This notebook is a tutorial on using AmpOpt to tune an amptorch model hyperparameters.\n",
    "\n",
    "Before starting this notebook, please make sure that you've followed all the steps in [SETUP.md](../docs/SETUP.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ampopt\n",
    "from ampopt.utils import format_params\n",
    "from ampopt.study import get_study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create MySQL Port\n",
    "\n",
    "In order to run hyperparameter tuning jobs, we need a MySQL port.\n",
    "\n",
    "update \".env\" if ssh is required include the last five argument in env\n",
    "\n",
    "```\n",
    "MYSQL_USERNAME=\n",
    "MYSQL_PASSWORD=\n",
    "HPOPT_DB=\n",
    "MYSQL_HOSTNAME=\n",
    "\n",
    "MYSQL_PORT=\n",
    "SSH_HOST=\n",
    "SSH_USER=\n",
    "SSH_PASS=\n",
    "SSH_PORT=\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "AmpOpt requires data to be preprocessed using the preferred fingerprinting scheme and preprocessing pipeline, and saved in LMDB format, before hyperparameter optimization. This saves a lot of work being wasted performing the featurization for every optimization trial.\n",
    "\n",
    "With AmpOpt, preprocessing and saving to LMDB is as easy as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ampopt.preprocess(\"../data/oc20_3k_train.traj\", \"../data/oc20_300_test.traj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data should be readable by either `ase.io.Trajectory` or `ase.io.read`. \n",
    "\n",
    "If you have several files, the first will be used to fit the transformers (e.g. for feature scaling). This prevents data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running an Individual Training Job\n",
    "\n",
    "Before we launch into running hyperparameter tuning jobs, let's train an individual model and evaluate it to get a (poor) baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ampopt.eval_score(\n",
    "    epochs=10,\n",
    "    train_fname=\"../data/oc20_3k_train.lmdb\",\n",
    "    valid_fname=\"../data/oc20_300_test.traj\",\n",
    "    dropout_rate=0.,\n",
    "    lr=1e-3,\n",
    "    gamma=1.,\n",
    "    num_nodes=5,\n",
    "    num_layers=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of this model is poor, but that's to be expected: we only trained it for 10 epochs. We'll improve this score in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running Single Tuning Jobs\n",
    "\n",
    "Let's first run a single tuning job to try and find the optimal number of layers and number of nodes per layer when training for just 10 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need to supply a single dataset; amptorch will split 10% of the data off as a validation set.\n",
    "\n",
    "The `study` argument can be anything, though we should be careful not to name this study the same as a previous study. It's how we'll later retrieve the study to perform analysis.\n",
    "\n",
    "For `params`, we can pass any of the following hyperparameters:\n",
    "\n",
    "- Learnable Parameters:\n",
    "    - `num_layers`, the number of layers of the neural network\n",
    "    - `num_nodes`, the number of nodes per layer\n",
    "    - `dropout_rate`, the rate of [dropout](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/) during training\n",
    "    - `lr`, the learning rate for gradient descent\n",
    "    - `gamma`, the decay parameter for the learning rate.\n",
    "- Non-Learnable Parameters:\n",
    "    - `step_size`, the number of epochs after which the learning rate decreases by `gamma`\n",
    "    - `batch_size`, the size of minibatches for gradient descent\n",
    "\n",
    "Any learnable parameter not fixed in the `params` argument will be learned during hyperparameter optimization. Any non-learnable parameter will be given a default value.\n",
    "\n",
    "The learnable and non-learnable parameters, as well as default values in the amptorch config, are specified in `src/ampopt/train.py`. Feel free to tweak this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running hyperparam tuning with:\n",
      " - study_name: tutorial1\n",
      " - dataset: ../data/oc20_3k_train.lmdb\n",
      " - n_trials: 2\n",
      " - sampler: CmaEs\n",
      " - pruner: Hyperband\n",
      " - num epochs: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-07 01:45:10,920]\u001b[0m Using an existing study with name 'tutorial1' instead of creating a new one.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - params:\n",
      "   - dropout_rate: 0.0\n",
      "   - gamma: 1.0\n",
      "CmaEsSampler\n",
      "<optuna.pruners._hyperband.HyperbandPruner object at 0x000001B73B516190>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading from C:\\Users\\ryanc\\Desktop\\Student Files\\VIP\\Alex-Repo\\bdqm-hyperparam-tuning\\data\\oc20_3k_train.lmdb:  15%|▏|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ./checkpoints\\2022-11-07-01-45-11-de3b7ce5-7a51-4816-b6d4-a3dca5426142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading from C:\\Users\\ryanc\\Desktop\\Student Files\\VIP\\Alex-Repo\\bdqm-hyperparam-tuning\\data\\oc20_3k_train.lmdb: 100%|█|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: 3000 images\n",
      "Use Xavier initialization\n",
      "Loading model: 2911 parameters\n",
      "Loading skorch trainer\n",
      "Training completed in 0.7639632225036621s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-07 01:45:13,338]\u001b[0m Trial 1 finished with value: 916.1633675160737 and parameters: {'num_layers': 11, 'num_nodes': 15, 'lr': 0.031263973163183786}. Best is trial 1 with value: 916.163.\u001b[0m\n",
      "loading from C:\\Users\\ryanc\\Desktop\\Student Files\\VIP\\Alex-Repo\\bdqm-hyperparam-tuning\\data\\oc20_3k_train.lmdb:  15%|▏|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ./checkpoints\\2022-11-07-01-45-13-ce1f3761-b136-40a3-89c2-32281b8ad1e0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading from C:\\Users\\ryanc\\Desktop\\Student Files\\VIP\\Alex-Repo\\bdqm-hyperparam-tuning\\data\\oc20_3k_train.lmdb: 100%|█|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: 3000 images\n",
      "Use Xavier initialization\n",
      "Loading model: 5207 parameters\n",
      "Loading skorch trainer\n",
      "Training completed in 0.9900028705596924s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-07 01:45:15,674]\u001b[0m Trial 2 finished with value: 1189.3425757743673 and parameters: {'num_layers': 13, 'num_nodes': 19, 'lr': 0.04222790494670366}. Best is trial 1 with value: 916.163.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ampopt.tune(\n",
    "    study=\"tutorial1\",\n",
    "    trials=10,\n",
    "    epochs=10,\n",
    "    data=\"../data/oc20_3k_train.lmdb\",\n",
    "    params=format_params(\n",
    "        dropout_rate=0.0,\n",
    "        gamma=1.0,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the tuning job is finished, we can load the study as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tutorial1 = get_study(\"tutorial1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the trials we ran:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_lr</th>\n",
       "      <th>params_num_layers</th>\n",
       "      <th>params_num_nodes</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-11-07 01:44:58</td>\n",
       "      <td>2022-11-07 01:45:05</td>\n",
       "      <td>0 days 00:00:07</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>10</td>\n",
       "      <td>29</td>\n",
       "      <td>FAIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>916.163</td>\n",
       "      <td>2022-11-07 01:45:11</td>\n",
       "      <td>2022-11-07 01:45:13</td>\n",
       "      <td>0 days 00:00:02</td>\n",
       "      <td>0.031264</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1189.340</td>\n",
       "      <td>2022-11-07 01:45:13</td>\n",
       "      <td>2022-11-07 01:45:15</td>\n",
       "      <td>0 days 00:00:02</td>\n",
       "      <td>0.042228</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number     value      datetime_start   datetime_complete        duration  \\\n",
       "0       0       NaN 2022-11-07 01:44:58 2022-11-07 01:45:05 0 days 00:00:07   \n",
       "1       1   916.163 2022-11-07 01:45:11 2022-11-07 01:45:13 0 days 00:00:02   \n",
       "2       2  1189.340 2022-11-07 01:45:13 2022-11-07 01:45:15 0 days 00:00:02   \n",
       "\n",
       "   params_lr  params_num_layers  params_num_nodes     state  \n",
       "0   0.002113                 10                29      FAIL  \n",
       "1   0.031264                 11                15  COMPLETE  \n",
       "2   0.042228                 13                19  COMPLETE  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tutorial1.trials_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parallel Tuning Jobs\n",
    "\n",
    "Of course, for optimizing over a large hyperparameter search space, we will want to parallelize our jobs. Doing this with AmpOpt is easy: simply add the `jobs` argument. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ampopt.tune(\n",
    "    study=\"tutorial2\",\n",
    "    trials=10,\n",
    "    epochs=10,\n",
    "    data=\"../data/oc20_3k_train.lmdb\",\n",
    "    params=format_params(\n",
    "        dropout_rate=0.0,\n",
    "        gamma=1.0,\n",
    "    ),\n",
    "    jobs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reports and Summaries\n",
    "\n",
    "To get a summary of all studies currently in the database, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study tutorial1:\n",
      "  Params:\n",
      "    - lr\n",
      "    - num_layers\n",
      "    - num_nodes\n",
      "  Best score: 916.163\n",
      "  Num trials: 3\n"
     ]
    }
   ],
   "source": [
    "ampopt.view_studies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a particular study, you can load it into memory and use `optuna.visualization.matplotlib` to easily visualise the study.\n",
    "\n",
    "AmpOpt provides a single function for generating several interesting plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report directory C:\\Users\\ryanc\\Desktop\\Student Files\\VIP\\Alex-Repo\\bdqm-hyperparam-tuning\\report\\tutorial1 already exists.\n"
     ]
    }
   ],
   "source": [
    "ampopt.generate_report(\"tutorial1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then view the generated plots in the `reports` folder of the project root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, perhaps you have run some experiments that aren't useful, and you'd like to clean up the list of studies. Run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted study tutorial1.\n"
     ]
    }
   ],
   "source": [
    "ampopt.delete_studies(\"tutorial1\", \"tutorial2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
