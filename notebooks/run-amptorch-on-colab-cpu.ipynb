{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "id": "UzNG76J60y_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z54O8hYIwGNM",
        "outputId": "0cc86bcf-8efe-4310-e5b2-15020b5cb7fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'amptorch' already exists and is not an empty directory.\n",
            "Cloning into 'bdqm-vip'...\n",
            "remote: Enumerating objects: 694, done.\u001b[K\n",
            "remote: Counting objects: 100% (280/280), done.\u001b[K\n",
            "remote: Compressing objects: 100% (227/227), done.\u001b[K\n",
            "remote: Total 694 (delta 157), reused 117 (delta 49), pack-reused 414\u001b[K\n",
            "Receiving objects: 100% (694/694), 143.31 MiB | 4.86 MiB/s, done.\n",
            "Resolving deltas: 100% (388/388), done.\n"
          ]
        }
      ],
      "source": [
        "!git config --global user.name \"Alex McKenzie\" \n",
        "!git config --global user.email \"amckenzie9@gatech.edu\"\n",
        "!git clone https://github.com/Arrrlex/amptorch.git && cd amptorch && git checkout fix-conda-env\n",
        "!git clone https://github.com/medford-group/bdqm-vip.git\n",
        "!mamba env update -n base -f amptorch/env_cpu.yml\n",
        "!pip install ./amptorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import ase.io\n",
        "from amptorch.trainer import AtomsTrainer\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "bdqm_path = Path(\"bdqm-vip\")\n",
        "amptorch_path = Path(\"amptorch\")\n",
        "\n",
        "train_data = ase.io.read(bdqm_path / \"data/amptorch_data/oc20_3k_train.traj\")\n",
        "\n",
        "elements = list(set(atom.symbol for atom in train_data))\n",
        "\n",
        "def get_path_to_gaussian(element):\n",
        "    gaussians_path = amptorch_path / \"examples/GMP/valence_gaussians\"\n",
        "    return next(p for p in gaussians_path.iterdir() if p.name.startswith(element + \"_\"))\n",
        "\n",
        "atom_gaussians = {element: get_path_to_gaussian(element) for element in elements}\n",
        "\n",
        "sigmas = [0.2, 0.69, 1.1, 1.66, 2.66]\n",
        "\n",
        "MCSHs = {\n",
        "    \"MCSHs\": {\n",
        "        \"0\": {\"groups\": [1], \"sigmas\": sigmas},\n",
        "        \"1\": {\"groups\": [1], \"sigmas\": sigmas},\n",
        "        \"2\": {\"groups\": [1, 2], \"sigmas\": sigmas},\n",
        "    },\n",
        "    \"atom_gaussians\": atom_gaussians,\n",
        "    \"cutoff\": 8,\n",
        "}\n",
        "\n",
        "train_data_path = str(bdqm_path / \"data/amptorch_data/oc20_3k_train.traj\")\n",
        "\n",
        "\n",
        "model_config = {\n",
        "    \"name\":\"singlenn\",\n",
        "    \"get_forces\": True,\n",
        "    \"num_layers\": 3,\n",
        "    \"num_nodes\": 10,\n",
        "    \"batchnorm\": True,\n",
        "}\n",
        "\n",
        "optim_config = {\n",
        "    \"force_coefficient\": 0.01,\n",
        "    \"lr\": 1e-3,\n",
        "    \"batch_size\": 16,\n",
        "    \"epochs\": 10,\n",
        "    \"loss\": \"mse\",\n",
        "    \"metric\": \"mae\",\n",
        "}\n",
        "\n",
        "dataset_config = {\n",
        "    \"raw_data\": train_data_path,\n",
        "    \"fp_scheme\": \"gmp\",\n",
        "    \"fp_params\": MCSHs,\n",
        "    \"elements\": elements,\n",
        "    \"save_fps\": True,\n",
        "    \"scaling\": {\"type\": \"normalize\", \"range\": (0, 1)},\n",
        "    \"val_split\": 0.1,\n",
        "}\n",
        "\n",
        "cmd_config = {\n",
        "    \"debug\": False,\n",
        "    \"run_dir\": \"./\",\n",
        "    \"seed\": 1,\n",
        "    \"identifier\": \"test\",\n",
        "    \"verbose\": True,\n",
        "    \"logger\": False,\n",
        "}\n",
        "\n",
        "config = {\n",
        "    \"model\": model_config,\n",
        "    \"optim\": optim_config,\n",
        "    \"dataset\": dataset_config,\n",
        "    \"cmd\": cmd_config,\n",
        "}\n",
        "\n",
        "torch.set_num_threads(1)\n",
        "trainer = AtomsTrainer(config)\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7B3DZAPU0mPk",
        "outputId": "524f21d4-3c6a-499c-d106-6b7f659dd3ea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to ./checkpoints/2022-02-08-15-14-05-test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "converting ASE atoms collection to Data objects: 100%|██████████| 3000/3000 [00:59<00:00, 50.18 systems/s]\n",
            "Scaling Feature data (normalize): 100%|██████████| 3000/3000 [01:09<00:00, 43.08 scalings/s]\n",
            "Scaling Target data: 100%|██████████| 3000/3000 [00:00<00:00, 41523.93 scalings/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset: 3000 images\n",
            "Loading model: 501 parameters\n",
            "Loading skorch trainer\n",
            "  epoch    train_energy_mae    train_forces_mae    train_loss    val_energy_mae    val_forces_mae    valid_loss    cp      dur\n",
            "-------  ------------------  ------------------  ------------  ----------------  ----------------  ------------  ----  -------\n",
            "      1           \u001b[36m1217.9261\u001b[0m            \u001b[32m112.0899\u001b[0m       \u001b[35m37.5123\u001b[0m         \u001b[31m1512.9265\u001b[0m           \u001b[94m86.5026\u001b[0m       \u001b[36m29.8566\u001b[0m     +  15.1829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      2            \u001b[36m626.4187\u001b[0m             \u001b[32m85.2256\u001b[0m        \u001b[35m7.9544\u001b[0m          \u001b[31m479.3150\u001b[0m           \u001b[94m71.5122\u001b[0m        \u001b[36m4.6715\u001b[0m     +  15.2036\n",
            "      3            \u001b[36m540.5634\u001b[0m             \u001b[32m66.5865\u001b[0m        \u001b[35m5.8387\u001b[0m          \u001b[31m436.3370\u001b[0m           \u001b[94m57.6336\u001b[0m        \u001b[36m3.4785\u001b[0m     +  15.1625\n",
            "      4            \u001b[36m467.2123\u001b[0m             \u001b[32m58.9983\u001b[0m        \u001b[35m4.0027\u001b[0m          \u001b[31m316.2756\u001b[0m           58.0438        \u001b[36m2.5031\u001b[0m     +  15.4901\n",
            "      5            \u001b[36m402.9479\u001b[0m             \u001b[32m53.5226\u001b[0m        \u001b[35m3.0752\u001b[0m          528.6107           58.6039        3.6555        15.4318\n",
            "      6            \u001b[36m384.3636\u001b[0m             \u001b[32m48.1549\u001b[0m        \u001b[35m2.6971\u001b[0m          \u001b[31m270.6585\u001b[0m           \u001b[94m42.5644\u001b[0m        \u001b[36m1.6739\u001b[0m     +  15.4567\n",
            "      7            \u001b[36m328.0601\u001b[0m             \u001b[32m35.9508\u001b[0m        \u001b[35m2.0188\u001b[0m          \u001b[31m265.5656\u001b[0m           47.5287        \u001b[36m1.5250\u001b[0m     +  15.5408\n",
            "      8            \u001b[36m292.3840\u001b[0m             \u001b[32m32.9062\u001b[0m        \u001b[35m1.5602\u001b[0m          374.6311           \u001b[94m36.3406\u001b[0m        2.0441        15.6800\n",
            "      9            \u001b[36m272.9246\u001b[0m             \u001b[32m27.6154\u001b[0m        \u001b[35m1.3595\u001b[0m          468.9204           38.4870        2.5853        15.5046\n",
            "     10            \u001b[36m254.6083\u001b[0m             \u001b[32m24.0205\u001b[0m        \u001b[35m1.1690\u001b[0m          284.6321           \u001b[94m30.2283\u001b[0m        \u001b[36m1.1563\u001b[0m        15.5927\n",
            "Training completed in 154.54798865318298s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "igXMFoRw0ozC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}