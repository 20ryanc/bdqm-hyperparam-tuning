{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization on PACE\n",
    "\n",
    "This notebook is a tutorial on using AmpOpt to tune an amptorch model hyperparameters on the PACE cluster.\n",
    "\n",
    "Before starting this notebook, please make sure that you've followed all the steps in [SETUP.md](../docs/SETUP.md).\n",
    "\n",
    "Tip: open this notebook on a GPU-enabled PACE Jupyter job by running this command from the project root:\n",
    "\n",
    "```\n",
    "./gpu-notebook.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ampopt\n",
    "from ampopt.utils import format_params\n",
    "from ampopt.study import get_study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create MySQL Port\n",
    "\n",
    "In order to run hyperparameter tuning jobs, we need a MySQL port.\n",
    "\n",
    "update \".env\" if ssh is required include the last five argument in env\n",
    "\n",
    "```\n",
    "MYSQL_USERNAME=\n",
    "MYSQL_PASSWORD=\n",
    "HPOPT_DB=\n",
    "MYSQL_HOSTNAME=\n",
    "\n",
    "MYSQL_PORT=\n",
    "SSH_HOST=\n",
    "SSH_USER=\n",
    "SSH_PASS=\n",
    "SSH_PORT=\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "AmpOpt requires data to be preprocessed using the preferred fingerprinting scheme and preprocessing pipeline, and saved in LMDB format, before hyperparameter optimization. This saves a lot of work being wasted performing the featurization for every optimization trial.\n",
    "\n",
    "With AmpOpt, preprocessing and saving to LMDB is as easy as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ampopt.preprocess(\"../data/oc20_50k_alex.extxyz\", \"../data/oc20_300_test.traj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data should be readable by either `ase.io.Trajectory` or `ase.io.read`. \n",
    "\n",
    "If you have several files, the first will be used to fit the transformers (e.g. for feature scaling). This prevents data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running an Individual Training Job\n",
    "\n",
    "Before we launch into running hyperparameter tuning jobs, let's train an individual model and evaluate it to get a (poor) baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ampopt.eval_score(\n",
    "    epochs=10,\n",
    "    train_fname=\"../data/oc20_50k_alex.lmdb\",\n",
    "    valid_fname=\"../data/oc20_300_test.traj\",\n",
    "    dropout_rate=0.,\n",
    "    lr=1e-3,\n",
    "    gamma=1.,\n",
    "    num_nodes=5,\n",
    "    num_layers=5,\n",
    "    port=port,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of this model is poor, but that's to be expected: we only trained it for 10 epochs. We'll improve this score in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running Tuning Jobs on PACE\n",
    "\n",
    "Let's first run a single tuning job to try and find the optimal number of layers and number of nodes per layer when training for just 10 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need to supply a single dataset; amptorch will split 10% of the data off as a validation set.\n",
    "\n",
    "The `study` argument can be anything, though we should be careful not to name this study the same as a previous study. It's how we'll later retrieve the study to perform analysis.\n",
    "\n",
    "For `params`, we can pass any of the following hyperparameters:\n",
    "\n",
    "- Learnable Parameters:\n",
    "    - `num_layers`, the number of layers of the neural network\n",
    "    - `num_nodes`, the number of nodes per layer\n",
    "    - `dropout_rate`, the rate of [dropout](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/) during training\n",
    "    - `lr`, the learning rate for gradient descent\n",
    "    - `gamma`, the decay parameter for the learning rate.\n",
    "- Non-Learnable Parameters:\n",
    "    - `step_size`, the number of epochs after which the learning rate decreases by `gamma`\n",
    "    - `batch_size`, the size of minibatches for gradient descent\n",
    "\n",
    "Any learnable parameter not fixed in the `params` argument will be learned during hyperparameter optimization. Any non-learnable parameter will be given a default value.\n",
    "\n",
    "The learnable and non-learnable parameters, as well as default values in the amptorch config, are specified in `src/ampopt/train.py`. Feel free to tweak this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ampopt.run_pace_tuning_job(\n",
    "    study=\"tutorial1\",\n",
    "    trials=10,\n",
    "    epochs=10,\n",
    "    data=\"../data/oc20_50k_alex.lmdb\",\n",
    "    params=format_params(\n",
    "        dropout_rate=0.0,\n",
    "        gamma=1.0,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that our job was successfully submitted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ampopt.view_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three jobs are as follows:\n",
    "\n",
    "- The first job, `mysql`, is running MySQL\n",
    "- The second job, `pace-jupyter-not`, is running the Jupyter notebook instance\n",
    "- The third job, `tune-amptorch-hy`, is the tuning job we just triggered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the job is finished, it will disappear from `ampopt.view_jobs()`. It will generate 2 log files, one for the stdout and one for the stderr. It's worth checking the log files to verify that the job completed successfully.\n",
    "\n",
    "We can load the study as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tutorial1 = get_study(\"tutorial1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the trials we ran:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tutorial1.trials_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parallel Tuning Jobs\n",
    "\n",
    "Of course, for optimizing over a large hyperparameter search space, we will want to parallelize our jobs. Doing this with AmpOpt and PACE is easy: simply run `ampopt.run_pace_tuning_job()` several times. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    ampopt.run_pace_tuning_job(\n",
    "        study=\"tutorial2\",\n",
    "        trials=20,\n",
    "        epochs=100,\n",
    "        data=\"../data/oc20_50k_alex.lmdb\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reports and Summaries\n",
    "\n",
    "To get a summary of all studies currently in the database, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ampopt.view_studies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a particular study, you can load it into memory and use `optuna.visualization.matplotlib` to easily visualise the study.\n",
    "\n",
    "AmpOpt provides a single function for generating several interesting plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ampopt.generate_report(\"tutorial1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then view the generated plots in the `reports` folder of the project root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, perhaps you have run some experiments that aren't useful, and you'd like to clean up the list of studies. Run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ampopt.delete_studies(\"tutorial1\", \"tutorial2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ampopt.tune(\n",
    "    jobs=5,\n",
    "    study=\"50K-alex-local\",\n",
    "    trials=5,\n",
    "    epochs=100,\n",
    "    data=\"../data/oc20_50k_alex.lmdb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ampopt.delete_studies(\"50K-alex-local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting e.g. `jobs=2` in ampopt.tune would run 2 processes,\n",
    "but on PACE it's more efficient to run several jobs instead:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
