{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization on PACE\n",
    "\n",
    "This notebook is a tutorial on using AmpOpt to tune an amptorch model hyperparameters on the PACE cluster.\n",
    "\n",
    "Before starting this notebook, please make sure that you've followed all the steps in [SETUP.md](../docs/SETUP.md).\n",
    "\n",
    "Tip: open this notebook on a GPU-enabled PACE Jupyter job by running this command from the project root:\n",
    "\n",
    "```\n",
    "./gpu-notebook.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ampopt\n",
    "from ampopt.utils import format_params\n",
    "from ampopt.study import get_study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Starting MySQL\n",
    "\n",
    "In order to run hyperparameter tuning jobs on PACE, we need a separate job running MySQL in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting mysql job\n",
      "115629.sched-pace-ice.pace.gatech.edu\n",
      "Waiting for mysql job 115629 to start...\n",
      "Waiting for mysql job 115629 to start...\n",
      "mysql running, job ID: 115629\n"
     ]
    }
   ],
   "source": [
    "ampopt.ensure_mysql_running()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function checks if a MySQL job is already running, and if not it starts one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "AmpOpt requires data to be preprocessed using the preferred fingerprinting scheme and preprocessing pipeline, and saved in LMDB format, before hyperparameter optimization. This saves a lot of work being wasted performing the featurization for every optimization trial.\n",
    "\n",
    "With AmpOpt, preprocessing and saving to LMDB is as easy as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LMDBs from files /storage/home/hpaceice1/amckenzie9/bdqm-hyperparam-tuning/data/oc20_50k_alex.extxyz, /storage/home/hpaceice1/amckenzie9/bdqm-hyperparam-tuning/data/oc20_300_test.traj\n",
      "/storage/home/hpaceice1/amckenzie9/bdqm-hyperparam-tuning/data/oc20_50k_alex.lmdb already exists, aborting\n"
     ]
    }
   ],
   "source": [
    "ampopt.preprocess(\"../data/oc20_50k_alex.extxyz\", \"../data/oc20_300_test.traj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data should be readable by either `ase.io.Trajectory` or `ase.io.read`. \n",
    "\n",
    "If you have several files, the first will be used to fit the transformers (e.g. for feature scaling). This prevents data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running an Individual Training Job\n",
    "\n",
    "Before we launch into running hyperparameter tuning jobs, let's train an individual model and evaluate it to get a (poor) baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation data labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading from /storage/home/hpaceice1/amckenzie9/bdqm-hyperparam-tuning/data/oc20_50k_alex.lmdb:   1%|          | 481/50000 [00:00<00:10, 4808.01 images/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ./checkpoints/2022-04-19-15-50-11-16322358-7c5c-4945-b592-e4a998f2c7d6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading from /storage/home/hpaceice1/amckenzie9/bdqm-hyperparam-tuning/data/oc20_50k_alex.lmdb: 100%|██████████| 50000/50000 [00:11<00:00, 4467.90 images/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: 50000 images\n",
      "Use Xavier initialization\n",
      "Loading model: 291 parameters\n",
      "Loading skorch trainer\n",
      "  epoch    train_energy_mae    train_loss    cp      lr     dur\n",
      "-------  ------------------  ------------  ----  ------  ------\n",
      "      1            \u001b[36m288.7328\u001b[0m        \u001b[32m0.6193\u001b[0m     +  0.0010  1.8943\n",
      "      2            \u001b[36m151.0952\u001b[0m        \u001b[32m0.3248\u001b[0m     +  0.0010  1.6754\n",
      "      3            \u001b[36m147.0156\u001b[0m        \u001b[32m0.3158\u001b[0m     +  0.0010  1.6832\n",
      "      4            \u001b[36m144.9110\u001b[0m        \u001b[32m0.3114\u001b[0m     +  0.0010  1.6841\n",
      "      5            \u001b[36m142.7045\u001b[0m        \u001b[32m0.3065\u001b[0m     +  0.0010  1.6727\n",
      "      6            \u001b[36m141.7071\u001b[0m        \u001b[32m0.3045\u001b[0m     +  0.0010  1.6730\n",
      "      7            \u001b[36m139.3646\u001b[0m        \u001b[32m0.2996\u001b[0m     +  0.0010  1.6710\n",
      "      8            \u001b[36m137.6498\u001b[0m        \u001b[32m0.2959\u001b[0m     +  0.0010  1.6715\n",
      "      9            \u001b[36m135.7527\u001b[0m        \u001b[32m0.2917\u001b[0m     +  0.0010  1.6783\n",
      "     10            \u001b[36m134.1199\u001b[0m        \u001b[32m0.2882\u001b[0m     +  0.0010  1.6736\n",
      "Training completed in 17.203376054763794s\n",
      "Calculating predictions on validation data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ada95da59e64392a179521e61f5da9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='converting ASE atoms collection to Data objects', max=300…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55dec7149fd4376a03ef51da009c7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Scaling Feature data (normalize)', max=300.0, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ec861b96a5430499f3057d869d0ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Predicting', max=300.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "128.52952354382325"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ampopt.eval_score(\n",
    "    epochs=10,\n",
    "    train_fname=\"../data/oc20_50k_alex.lmdb\",\n",
    "    valid_fname=\"../data/oc20_300_test.traj\",\n",
    "    dropout_rate=0.,\n",
    "    lr=1e-3,\n",
    "    gamma=1.,\n",
    "    num_nodes=5,\n",
    "    num_layers=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of this model is poor, but that's to be expected: we only trained it for 10 epochs. We'll improve this score in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running Tuning Jobs on PACE\n",
    "\n",
    "Let's first run a single tuning job to try and find the optimal number of layers and number of nodes per layer when training for just 10 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need to supply a single dataset; amptorch will split 10% of the data off as a validation set.\n",
    "\n",
    "The `study` argument can be anything, though we should be careful not to name this study the same as a previous study. It's how we'll later retrieve the study to perform analysis.\n",
    "\n",
    "For `params`, we can pass any of the following hyperparameters:\n",
    "\n",
    "- Learnable Parameters:\n",
    "    - `num_layers`, the number of layers of the neural network\n",
    "    - `num_nodes`, the number of nodes per layer\n",
    "    - `dropout_rate`, the rate of [dropout](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/) during training\n",
    "    - `lr`, the learning rate for gradient descent\n",
    "    - `gamma`, the decay parameter for the learning rate.\n",
    "- Non-Learnable Parameters:\n",
    "    - `step_size`, the number of epochs after which the learning rate decreases by `gamma`\n",
    "    - `batch_size`, the size of minibatches for gradient descent\n",
    "\n",
    "Any learnable parameter not fixed in the `params` argument will be learned during hyperparameter optimization. Any non-learnable parameter will be given a default value.\n",
    "\n",
    "The learnable and non-learnable parameters, as well as default values in the amptorch config, are specified in `src/ampopt/train.py`. Feel free to tweak this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mysql running, job ID: 115629\n",
      "115650.sched-pace-ice.pace.gatech.edu\n"
     ]
    }
   ],
   "source": [
    "ampopt.run_pace_tuning_job(\n",
    "    study=\"tutorial1\",\n",
    "    trials=10,\n",
    "    epochs=10,\n",
    "    data=\"../data/oc20_50k_alex.lmdb\",\n",
    "    params=format_params(\n",
    "        dropout_rate=0.0,\n",
    "        gamma=1.0,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that our job was successfully submitted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id   username    queue             name sessid nds tsk memory     time status  elapsed             node\n",
      "115629 amckenzie9 pace-ice            mysql 139625   1   1     -- 08:00:00      R 06:01:16 atl1-1-02-009-31\n",
      "115643 amckenzie9 pace-ice pace-jupyter-not 186065   1   1     -- 03:00:00      R 00:38:16 atl1-1-02-009-31\n",
      "115650 amckenzie9 pace-ice tune-amptorch-hy     --   1   1    2gb 02:00:00      Q       --               --\n"
     ]
    }
   ],
   "source": [
    "ampopt.view_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three jobs are as follows:\n",
    "\n",
    "- The first job, `mysql`, is running MySQL\n",
    "- The second job, `pace-jupyter-not`, is running the Jupyter notebook instance\n",
    "- The third job, `tune-amptorch-hy`, is the tuning job we just triggered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the job is finished, it will disappear from `ampopt.view_jobs()`. It will generate 2 log files, one for the stdout and one for the stderr. It's worth checking the log files to verify that the job completed successfully.\n",
    "\n",
    "We can load the study as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tutorial1 = get_study(\"tutorial1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the trials we ran:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_lr</th>\n",
       "      <th>params_num_layers</th>\n",
       "      <th>params_num_nodes</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>120.043</td>\n",
       "      <td>2022-04-19 16:04:11</td>\n",
       "      <td>2022-04-19 16:05:15</td>\n",
       "      <td>0 days 00:01:04</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>17</td>\n",
       "      <td>26</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>119.026</td>\n",
       "      <td>2022-04-19 16:05:15</td>\n",
       "      <td>2022-04-19 16:05:45</td>\n",
       "      <td>0 days 00:00:30</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>136.510</td>\n",
       "      <td>2022-04-19 16:05:45</td>\n",
       "      <td>2022-04-19 16:06:20</td>\n",
       "      <td>0 days 00:00:35</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>109.859</td>\n",
       "      <td>2022-04-19 16:06:20</td>\n",
       "      <td>2022-04-19 16:06:49</td>\n",
       "      <td>0 days 00:00:29</td>\n",
       "      <td>0.005522</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>107.000</td>\n",
       "      <td>2022-04-19 16:06:49</td>\n",
       "      <td>2022-04-19 16:07:22</td>\n",
       "      <td>0 days 00:00:33</td>\n",
       "      <td>0.005742</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>113.928</td>\n",
       "      <td>2022-04-19 16:07:22</td>\n",
       "      <td>2022-04-19 16:07:51</td>\n",
       "      <td>0 days 00:00:29</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-04-19 16:07:51</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>RUNNING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number    value      datetime_start   datetime_complete        duration  \\\n",
       "0       0  120.043 2022-04-19 16:04:11 2022-04-19 16:05:15 0 days 00:01:04   \n",
       "1       1  119.026 2022-04-19 16:05:15 2022-04-19 16:05:45 0 days 00:00:30   \n",
       "2       2  136.510 2022-04-19 16:05:45 2022-04-19 16:06:20 0 days 00:00:35   \n",
       "3       3  109.859 2022-04-19 16:06:20 2022-04-19 16:06:49 0 days 00:00:29   \n",
       "4       4  107.000 2022-04-19 16:06:49 2022-04-19 16:07:22 0 days 00:00:33   \n",
       "5       5  113.928 2022-04-19 16:07:22 2022-04-19 16:07:51 0 days 00:00:29   \n",
       "6       6      NaN 2022-04-19 16:07:51                 NaT             NaT   \n",
       "\n",
       "   params_lr  params_num_layers  params_num_nodes     state  \n",
       "0   0.000064                 17                26  COMPLETE  \n",
       "1   0.000516                  9                17  COMPLETE  \n",
       "2   0.000014                 17                17  COMPLETE  \n",
       "3   0.005522                  8                11  COMPLETE  \n",
       "4   0.005742                 14                27  COMPLETE  \n",
       "5   0.000646                  7                23  COMPLETE  \n",
       "6   0.000070                 14                17   RUNNING  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tutorial1.trials_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parallel Tuning Jobs\n",
    "\n",
    "Of course, for optimizing over a large hyperparameter search space, we will want to parallelize our jobs. Doing this with AmpOpt and PACE is easy: simply run `ampopt.run_pace_tuning_job()` several times. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mysql running, job ID: 115629\n",
      "115645.sched-pace-ice.pace.gatech.edu\n",
      "mysql running, job ID: 115629\n",
      "115646.sched-pace-ice.pace.gatech.edu\n",
      "mysql running, job ID: 115629\n",
      "115647.sched-pace-ice.pace.gatech.edu\n",
      "mysql running, job ID: 115629\n",
      "115648.sched-pace-ice.pace.gatech.edu\n",
      "mysql running, job ID: 115629\n",
      "115649.sched-pace-ice.pace.gatech.edu\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    ampopt.run_pace_tuning_job(\n",
    "        study=\"tutorial2\",\n",
    "        trials=20,\n",
    "        epochs=100,\n",
    "        data=\"../data/oc20_50k_alex.lmdb\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reports and Summaries\n",
    "\n",
    "To get a summary of all studies currently in the database, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study 50K-alex-with-lr-and-gamma:\n",
      "  Params:\n",
      "    - gamma\n",
      "    - lr\n",
      "    - num_layers\n",
      "    - num_nodes\n",
      "  Best score: 68.9657\n",
      "  Num trials: 150\n",
      "Study cmaes-oc20-3k:\n",
      "  Params:\n",
      "    - lr\n",
      "    - num_layers\n",
      "    - num_nodes\n",
      "  Best score: 91.3529\n",
      "  Num trials: 60\n",
      "Study random-oc20-3k:\n",
      "  Params:\n",
      "    - lr\n",
      "    - num_layers\n",
      "    - num_nodes\n",
      "  Best score: 90.7533\n",
      "  Num trials: 60\n",
      "Study tpe-oc20-3k:\n",
      "  Params:\n",
      "    - lr\n",
      "    - num_layers\n",
      "    - num_nodes\n",
      "  Best score: 91.9822\n",
      "  Num trials: 60\n",
      "Study tutorial1:\n",
      "  Params:\n",
      "    - lr\n",
      "    - num_layers\n",
      "    - num_nodes\n",
      "  Best score: 106.676\n",
      "  Num trials: 4\n"
     ]
    }
   ],
   "source": [
    "ampopt.view_studies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a particular study, you can load it into memory and use `optuna.visualization.matplotlib` to easily visualise the study.\n",
    "\n",
    "AmpOpt provides a single function for generating several interesting plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2022-04-19 16:15:17,077]\u001b[0m Param num_nodes unique value length is less than 2.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'lr': 0.00112022, 'num_layers': 17, 'num_nodes': 21} with MAE 111.756\n",
      "Report saved to /storage/home/hpaceice1/amckenzie9/bdqm-hyperparam-tuning/report/tutorial1\n"
     ]
    }
   ],
   "source": [
    "ampopt.generate_report(\"tutorial1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then view the generated plots in the `reports` folder of the project root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, perhaps you have run some experiments that aren't useful, and you'd like to clean up the list of studies. Run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted study tutorial1.\n",
      "Deleted study tutorial2.\n"
     ]
    }
   ],
   "source": [
    "ampopt.delete_studies(\"tutorial1\", \"tutorial2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-19 03:26:19,472]\u001b[0m A new study created in RDB with name: 50K-alex\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running hyperparam tuning with:\n",
      " - study_name: 50K-alex\n",
      " - dataset: ../data/oc20_50k_alex.lmdb\n",
      " - n_trials: 2\n",
      " - sampler: CmaEs\n",
      " - pruner: Median\n",
      " - num epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading from /storage/home/hpaceice1/amckenzie9/bdqm-hyperparam-tuning/data/oc20_50k_alex.lmdb:   0%|          | 0/50000 [00:00<?, ? images/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - params:\n",
      "   - dropout_rate: 0.0\n",
      "   - gamma: 1.0\n",
      "   - lr: 0.001\n",
      "Results saved to ./checkpoints/2022-04-19-03-26-19-13e6986f-ff7c-4686-a28b-43276c760144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading from /storage/home/hpaceice1/amckenzie9/bdqm-hyperparam-tuning/data/oc20_50k_alex.lmdb: 100%|██████████| 50000/50000 [00:11<00:00, 4465.65 images/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: 50000 images\n",
      "Use Xavier initialization\n",
      "Loading model: 6085 parameters\n",
      "Loading skorch trainer\n",
      "  epoch    train_energy_mae    train_loss    val_energy_mae    valid_loss    cp      lr     dur\n",
      "-------  ------------------  ------------  ----------------  ------------  ----  ------  ------\n",
      "      1            \u001b[36m225.0493\u001b[0m        \u001b[32m0.4833\u001b[0m          \u001b[35m134.0000\u001b[0m        \u001b[31m0.2880\u001b[0m     +  0.0010  2.4851\n",
      "      2            \u001b[36m137.3736\u001b[0m        \u001b[32m0.2951\u001b[0m          142.5463        0.3062        0.0010  2.2722\n",
      "      3            \u001b[36m134.1583\u001b[0m        \u001b[32m0.2883\u001b[0m          140.4345        0.3021        0.0010  2.2691\n",
      "      4            \u001b[36m132.0226\u001b[0m        \u001b[32m0.2837\u001b[0m          \u001b[35m125.1809\u001b[0m        \u001b[31m0.2691\u001b[0m     +  0.0010  2.2657\n",
      "      5            \u001b[36m127.5917\u001b[0m        \u001b[32m0.2742\u001b[0m          \u001b[35m120.0584\u001b[0m        \u001b[31m0.2583\u001b[0m     +  0.0010  2.2666\n",
      "      6            128.2016        0.2755          123.3189        0.2651        0.0010  2.2649\n",
      "      7            \u001b[36m125.4709\u001b[0m        \u001b[32m0.2696\u001b[0m          120.7507        0.2597        0.0010  2.2687\n",
      "      8            \u001b[36m123.4781\u001b[0m        \u001b[32m0.2653\u001b[0m          \u001b[35m115.4484\u001b[0m        \u001b[31m0.2483\u001b[0m     +  0.0010  2.2603\n",
      "      9            \u001b[36m123.3500\u001b[0m        \u001b[32m0.2651\u001b[0m          \u001b[35m114.2894\u001b[0m        \u001b[31m0.2458\u001b[0m     +  0.0010  2.2630\n",
      "     10            \u001b[36m120.9796\u001b[0m        \u001b[32m0.2599\u001b[0m          \u001b[35m113.8931\u001b[0m        \u001b[31m0.2448\u001b[0m     +  0.0010  2.2647\n",
      "Training completed in 24.091713905334473s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-19 03:26:57,810]\u001b[0m Trial 0 finished with value: 113.89310749326707 and parameters: {'num_layers': 17, 'num_nodes': 18}. Best is trial 0 with value: 113.893.\u001b[0m\n",
      "loading from /storage/home/hpaceice1/amckenzie9/bdqm-hyperparam-tuning/data/oc20_50k_alex.lmdb:   1%|          | 454/50000 [00:00<00:10, 4534.73 images/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ./checkpoints/2022-04-19-03-26-57-dcaeb72b-5c50-4caa-8823-3269af3ab316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading from /storage/home/hpaceice1/amckenzie9/bdqm-hyperparam-tuning/data/oc20_50k_alex.lmdb: 100%|██████████| 50000/50000 [00:10<00:00, 4585.89 images/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: 50000 images\n",
      "Use Xavier initialization\n",
      "Loading model: 8001 parameters\n",
      "Loading skorch trainer\n",
      "  epoch    train_energy_mae    train_loss    val_energy_mae    valid_loss    cp      lr     dur\n",
      "-------  ------------------  ------------  ----------------  ------------  ----  ------  ------\n",
      "      1            \u001b[36m190.3676\u001b[0m        \u001b[32m0.4089\u001b[0m          \u001b[35m139.4427\u001b[0m        \u001b[31m0.2996\u001b[0m     +  0.0010  2.0187\n",
      "      2            \u001b[36m134.3880\u001b[0m        \u001b[32m0.2887\u001b[0m          \u001b[35m130.9475\u001b[0m        \u001b[31m0.2816\u001b[0m     +  0.0010  2.0094\n",
      "      3            \u001b[36m132.1665\u001b[0m        \u001b[32m0.2840\u001b[0m          \u001b[35m127.2361\u001b[0m        \u001b[31m0.2735\u001b[0m     +  0.0010  2.0090\n",
      "      4            \u001b[36m129.9690\u001b[0m        \u001b[32m0.2793\u001b[0m          \u001b[35m126.0410\u001b[0m        \u001b[31m0.2711\u001b[0m     +  0.0010  2.0163\n",
      "      5            \u001b[36m129.0102\u001b[0m        \u001b[32m0.2772\u001b[0m          \u001b[35m121.9872\u001b[0m        \u001b[31m0.2623\u001b[0m     +  0.0010  2.0125\n",
      "      6            \u001b[36m125.5399\u001b[0m        \u001b[32m0.2698\u001b[0m          122.7576        0.2639        0.0010  2.0086\n",
      "      7            \u001b[36m123.8439\u001b[0m        \u001b[32m0.2661\u001b[0m          \u001b[35m116.0681\u001b[0m        \u001b[31m0.2496\u001b[0m     +  0.0010  2.0064\n",
      "      8            \u001b[36m121.3388\u001b[0m        \u001b[32m0.2607\u001b[0m          \u001b[35m113.8168\u001b[0m        \u001b[31m0.2447\u001b[0m     +  0.0010  2.0113\n",
      "      9            \u001b[36m118.6307\u001b[0m        \u001b[32m0.2549\u001b[0m          \u001b[35m113.4701\u001b[0m        \u001b[31m0.2442\u001b[0m     +  0.0010  2.0145\n",
      "     10            120.3775        0.2587          \u001b[35m111.7109\u001b[0m        \u001b[31m0.2403\u001b[0m     +  0.0010  2.0126\n",
      "Training completed in 22.203055381774902s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-19 03:27:31,342]\u001b[0m Trial 1 finished with value: 111.71088543808824 and parameters: {'num_layers': 12, 'num_nodes': 25}. Best is trial 1 with value: 111.711.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ampopt.tune(\n",
    "    jobs=5,\n",
    "    study=\"50K-alex-local\",\n",
    "    trials=5,\n",
    "    epochs=100,\n",
    "    data=\"../data/oc20_50k_alex.lmdb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ampopt.delete_studies(\"50K-alex-local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting e.g. `jobs=2` in ampopt.tune would run 2 processes,\n",
    "but on PACE it's more efficient to run several jobs instead:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
